
from typing import TypedDict, Annotated, Literal
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_community.utilities import GoogleSerperAPIWrapper
import operator
import os
from langchain.chat_models import init_chat_model
from datetime import datetime
from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain.prompts import ChatPromptTemplate
from typing import Dict, List

from src.config.settings import settings


class kw_res(BaseModel):
    """æœç´¢ç»“æžœçš„å®Œæ•´æ¨¡åž‹"""
    correct_nicknames: List[str] = Field(
        default=[],
        description="å“ç‰Œæˆ–äº§å“çš„æœ‰æ•ˆæ˜µç§°åˆ—è¡¨",
        example=["æ¬§èŽ±é›…", "å·´é»Žæ¬§èŽ±é›…", "L'Oreal Paris"]
    )
    correct_urls: List[str] = Field(
        default=[],
        description="æœ‰æ•ˆæ˜µç§°çš„å‚è€ƒé“¾æŽ¥URL",
        example=[r"https://www.zhihu.com/question/12345", "xxxxx.com"]
    )
    incorrect_nicknames: List[str] = Field(
        default=[],
        description="å“ç‰Œæˆ–äº§å“çš„é”™è¯¯çš„æ˜µç§°åˆ—è¡¨",
        example=["é”™è¯¯æ˜µç§°1", "é”™è¯¯æ˜µç§°2", "é”™è¯¯æ˜µç§°3"]
    )
    incorrect_urls: List[str] = Field(
        default=[],
        description="é”™è¯¯æ˜µç§°çš„å‚è€ƒé“¾æŽ¥URL",
        example=["é”™è¯¯é“¾æŽ¥1", "é”™è¯¯é“¾æŽ¥2"]
    )


class AnalystResponseFormatter(BaseModel):
    """Always use this tool to structure your response to the user."""
    next_action: Literal["[FINAL_RESULT]", "[REQUEST_MORE_INFO]"] = Field(
        description="ä¸‹ä¸€æ­¥ã€‚å¦‚æžœä¸éœ€è¦è¡¥å……ä¿¡æ¯äº†ï¼Œå°±è¿”å›ž[FINAL_RESULT]ã€‚å¦‚æžœéœ€è¦è¡¥å……ä¿¡æ¯ï¼Œè¿”å›ž[REQUEST_MORE_INFO]"
    )
    additional_info: str = Field(description="å¦‚æžœéœ€è¦è¡¥å……ä¿¡æ¯[REQUEST_MORE_INFO]ï¼Œåˆ—å‡ºéœ€è¦è¡¥å……çš„ä¿¡æ¯æˆ–æŸ¥è¯¢æ–¹å‘")
    result: kw_res = Field(description="æœ€ç»ˆç»“æžœç›¸å…³çš„å“ç‰Œæˆ–å•†å“åå­—å’Œé“¾æŽ¥")

parser = PydanticOutputParser(pydantic_object=AnalystResponseFormatter)
format_inst=parser.get_format_instructions()

# Define the shared state
class AgentState(TypedDict):
    industry: str
    subject: str
    query: Annotated[list, operator.add]
    messages: Annotated[list, operator.add]
    research_data: Annotated[list, operator.add]
    final_result: str
    next_agent: str
    iteration_count: int


# Initialize the LLM and Serper search tool
llm = init_chat_model(model=settings.MODEL_NAME, model_provider=settings.MODEL_PROVIDER, temperature=settings.TEMPERATURE)
llm2 = init_chat_model(model=settings.MODEL_NAME, model_provider=settings.MODEL_PROVIDER, temperature=settings.TEMPERATURE)
search = GoogleSerperAPIWrapper(k=10)

def perform_search(query: str) -> str:
    """
    Perform a web search using Serper API
    """
    try:
        results = search.results(query)

        # Extract relevant information
        search_results = []

        # Add organic results
        if "organic" in results:
            for i, result in enumerate(results["organic"][:5], 1):
                search_results.append(f"""Result {i}:
                Title: {result.get('title', 'N/A')}
                Link: {result.get('link', 'N/A')}
                Snippet: {result.get('snippet', 'N/A')}""")

        # Add knowledge graph if available
        if "knowledgeGraph" in results:
            kg = results["knowledgeGraph"]
            search_results.append(f"""Knowledge Graph:
            Title: {kg.get('title', 'N/A')}
            Description: {kg.get('description', 'N/A')}""")

        # Add answer box if available
        if "answerBox" in results:
            ab = results["answerBox"]
            search_results.append(f"""
            Answer Box:{ab.get('answer', ab.get('snippet', 'N/A'))}""")

        return "\n".join(search_results) if search_results else "No results found"

    except Exception as e:
        return f"Search error: {str(e)}"


def researcher_node(state: AgentState) -> AgentState:
    """
    Researcher agent that performs searches using Serper API
    """
    print("\nðŸ” RESEARCHER AGENT ACTIVE")

    # Get the last message to understand what to research
    messages = state.get("messages", [])
    subject = state.get("subject", "")
    industry = state.get("industry", "")
    previous_query = state.get("query", "")

    system_prompt1 = f'''
        ## è§’è‰²ï¼š
        ä½ æ˜¯ä¸€ä½å–„ç”¨æœç´¢å·¥å…·çš„ç ”ç©¶å‘˜ã€‚ä½ éœ€è¦æ ¹æ®ç”¨æˆ·æä¾›çš„ä¿¡æ¯è¿›è¡Œæ”¹å†™å’Œè°ƒæ•´ï¼Œå¹¶ç”¨è¿™è°ƒæ•´å¥½çš„ä¿¡æ¯è¿›è¡ŒæŸ¥è¯¢ã€‚
        
        ## ä»»åŠ¡ï¼š
        ä½ æ˜¯{industry}è¡Œä¸šä¸“å®¶ï¼Œç›®å‰åœ¨æ•´åˆ{industry}è¡Œä¸šå“ç‰Œå’Œäº§å“çš„å„ç§ä¸­è‹±æ–‡æ˜µç§°æˆ–åå­—ã€‚
        æ ¹æ®æä¾›çš„{industry}è¡Œä¸šå“ç‰Œæˆ–äº§å“åˆ—è¡¨ï¼Œç”¨æœç´ å·¥å…·æœç´ è¿™äº›å“ç‰Œæˆ–äº§å“çš„å…¶ä»–æ˜µç§°å’Œåå­—ã€‚å°½å¯èƒ½å…¨ã€‚
        
        ## ä¿¡æ¯ï¼š
        ç”¨æˆ·æä¾›çš„ä¿¡æ¯æ˜¯ï¼š{subject};
        ç”¨æˆ·æä¾›çš„è¡Œä¸šæ˜¯ï¼š{industry};
        
        ## åŽŸåˆ™ï¼š
        1ã€åªè¿”å›žæœç´¢æŸ¥è¯¢å¥å­ï¼Œåˆ«çš„éƒ½ä¸è¦ã€‚
        2ã€å¯ä»¥è€ƒè™‘ç”¨ â€œXXXçš„å…¶ä»–æ˜µç§°â€ã€â€å“ç‰ŒXXXçš„å…¶ä»–åå­—â€œã€â€äº§å“Aæœ‰å…¶ä»–åå­—å˜›ï¼Ÿâ€œ ç±»ä¼¼è¿™æ ·çš„ä¸­è‹±æ–‡å¥å­è¿›è¡Œæœç´¢ã€‚'''

    response = llm.invoke([SystemMessage(content=system_prompt1)])

    query_content = response.content

    # Determine search query
    if not messages or len(messages) == 0:
        # Initial research
        search_query = query_content
        print(f"ç¬¬ä¸€ä¸ªæŸ¥è¯¢: {search_query}")
    else:
        # Follow-up research based on analyst's request
        last_message = messages[-1]

        # Use LLM to extract specific search query from analyst's request
        query_prompt = f"""æ ¹æ®åˆ†æžå¸ˆçš„éœ€æ±‚åé¦ˆï¼Œè¯·å†ç”Ÿæˆ1ä¸ªç²¾ç®€çš„ç½‘ç»œæœç´¢æŸ¥è¯¢å¥å­ã€‚ç¡®ä¿æœç´¢æŸ¥è¯¢å¥å­çš„å­—æ•°ä¸è¶…è¿‡50ä¸ªå­—.
        
        åˆ†æžå¸ˆåé¦ˆ: {last_message.content}
        åŽŸä¸»é¢˜éœ€æ±‚: {subject}
        ä¹‹å‰ä½ ç”¨è¿‡çš„æœç´¢æŸ¥è¯¢å¥å­ï¼š{previous_query}
        
        ## åŽŸåˆ™ï¼š
        1ã€åªè¿”å›žæœç´¢æŸ¥è¯¢å¥å­ï¼Œåˆ«çš„éƒ½ä¸è¦ã€‚
        2ã€é¿å…ä½¿ç”¨ä¹‹å‰ä½ ç”¨è¿‡çš„æœç´¢æŸ¥è¯¢å¥å­ã€‚"""

        query_response = llm.invoke([SystemMessage(content=system_prompt1),
                                     HumanMessage(content=query_prompt)])
        search_query = query_response.content.strip()
        print(f"Follow-up search query: {search_query}")

    # Perform actual web search using Serper
    print(f"Searching web for: {search_query}")
    search_results = perform_search(search_query)

    system_prompt2 = f'''
            ## è§’è‰²ï¼š
            ä½ ä½ æ˜¯ä¸€ä½å¸‚åœºè°ƒç ”ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯èƒ½å¾ˆå¥½çš„æ¢³ç†å’Œå½’çº³ä¿¡æ¯ã€‚

            ## ä»»åŠ¡ï¼š
            ä½ æ˜¯{industry}è¡Œä¸šä¸“å®¶ï¼Œç›®å‰åœ¨æ•´åˆ{industry}è¡Œä¸šå“ç‰Œå’Œäº§å“çš„å„ç§ä¸­è‹±æ–‡æ˜µç§°æˆ–åå­—ã€‚
            æ ¹æ®æœç´¢æŸ¥è¯¢å›žæ¥çš„ä¿¡æ¯ï¼Œä½ çš„ä»»åŠ¡æ˜¯æŠŠæ‰€æœ‰çš„å“ç‰Œæˆ–äº§å“ä¸­è‹±æ–‡åå­—å…¨åˆ—å‡ºæ¥ã€‚'''

    # Have LLM synthesize the search results
    synthesis_prompt = f"""è¯·æ ¹æ®ç½‘ç»œæœç´¢ç»“æžœè¿›è¡Œåˆ†æžå’ŒæŠŠæ‰€æœ‰å“ç‰Œ/äº§å“ä¸­è‹±æ–‡åå­—å’Œå¯¹åº”çš„é“¾æŽ¥åˆ—å‡ºæ¥ã€‚
    
    è¡Œä¸šï¼š{industry}
    åŽŸéœ€æ±‚ã€äº§å“/å“ç‰Œã€‘åï¼š{subject}
    æœç´¢æŸ¥è¯¢å¥å­ï¼š{search_query}
    æœç´¢æŸ¥è¯¢ç»“æžœï¼š{search_results}
    
    è¯·æ ¹æ®æœç´¢æŸ¥è¯¢ç»“æžœï¼ŒæŠŠæ‰€æœ‰å¯èƒ½çš„ã€äº§å“/å“ç‰Œã€‘ä¸­è‹±æ–‡åå­—å’Œå¯¹åº”çš„é“¾æŽ¥å…¨éƒ¨åˆ—å‡ºæ¥ï¼Œå¹¶æŒ‰ç¼–å·æŽ’åºã€‚"""

    response = llm.invoke([
        SystemMessage(content=system_prompt2),
        HumanMessage(content=synthesis_prompt)
    ])

    research_content = response.content
    print(f"æœç´¢åŽçš„ä¿¡æ¯æ€»ç»“: {research_content[:400]}...")

    return {
        "research_data": [research_content],
        "query":[query_content],
        "messages": [AIMessage(content=f"[RESEARCHER]: {research_content}", name="researcher")],
        "next_agent": "analyst"
    }

def analyst_node(state: AgentState) -> AgentState:
    """
    Analyst agent that reviews research and writes reports or requests more info
    """
    print("\nðŸ“Š ANALYST AGENT ACTIVE")

    research_data = state.get("research_data", [])
    subject = state.get("subject", "")
    industry = state.get("industry", "")
    iteration = state.get("iteration_count", 0)

    system_prompt = f'''
    ## è§’è‰²ï¼š
    ä½ æ˜¯{industry}è¡Œä¸šä¸“å®¶ï¼Œç›®å‰åœ¨æ•´åˆ{industry}è¡Œä¸šå“ç‰Œæˆ–äº§å“çš„å„ç§ä¸­è‹±æ–‡æ˜µç§°æˆ–åå­—ã€‚
    
    ## ä»»åŠ¡ï¼š
    1ã€å®¡é˜…ã€æŸ¥è¯¢ç ”ç©¶ä»£ç†ã€‘æä¾›çš„ç½‘ç»œæœç´¢ç»“æžœæ•°æ®ï¼ŒæŠŠæ‰€æœ‰å¯èƒ½çš„ä¸­è‹±æ–‡äº§å“æˆ–å“ç‰Œåå…¨éƒ¨åˆ—å‡ºæ¥ã€‚
    2ã€åˆ¤æ–­æ˜¯å¦å·²æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥åˆ—å‡ºæ‰€æœ‰å¯èƒ½çš„ä¸­è‹±æ–‡å“ç‰Œæˆ–äº§å“åå­—ã€‚
    3ã€å¦‚æžœä¿¡æ¯ä¸å¤Ÿï¼Œä½ éœ€è¦åé¦ˆç»™ã€æŸ¥è¯¢ç ”ç©¶ä»£ç†ã€‘ï¼Œè®©ä»–æŒ‰ç…§ä½ çš„åé¦ˆè¿›è¡Œä¸‹ä¸€æ­¥æŸ¥è¯¢ï¼Œä»¥è¡¥å…¨ä¿¡æ¯ã€‚
    
    è¡Œä¸šï¼š{industry}
    åŽŸéœ€æ±‚ã€äº§å“/å“ç‰Œã€‘åï¼š{subject}
    ç›®å‰æŸ¥è¯¢æ¬¡æ•°ï¼š{iteration}
    
    ## è¾“å‡ºæ ¼å¼ï¼š
    {format_inst}
    
    ## åŽŸåˆ™ï¼š
    1ã€æŒ‰ç…§ä»¥ä¸Šçš„jsonæ ¼å¼è¾“å‡ºã€‚
    2ã€è¯·ç¡®ä¿æœ€ç»ˆåˆ—è¡¨correct_nicknamesé‡Œæœ‰è¶³å¤Ÿå¤šçš„å“ç‰Œæˆ–äº§å“è‹±æ–‡å’Œä¸­æ–‡åå­—ã€‚å¿…é¡»æœ‰4ä¸ªè‹±æ–‡åå­—ï¼Œä¹Ÿå¿…é¡»æœ‰4ä¸ªä¸­æ–‡åå­—ã€‚æ€»å…±å¿…é¡»åœ¨10ä¸ªä»¥ä¸Šã€‚å¦‚æžœä¸æ»¡è¶³è¿™ä¸ªæ¡ä»¶ï¼Œè¯·æŒ‰ä¸Šè¿°çš„jsonæ ¼å¼è¾“å‡º[REQUEST_MORE_INFO]ï¼Œå¹¶åœ¨additional_infoé‡Œæå‡ºéœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢æ–¹å‘ã€‚
    3ã€ä¾‹å¦‚ï¼Œå¦‚æžœæœ€ç»ˆå“ç‰Œæˆ–äº§å“åå­—åˆ—è¡¨correct_nicknamesç»“æžœé‡Œçš„è‹±æ–‡åå­—å°‘äºŽ4ä¸ªï¼Œé‚£ä¹ˆè¯·æŒ‰ä¸Šè¿°çš„jsonæ ¼å¼è¾“å‡º[REQUEST_MORE_INFO]ï¼Œå¹¶åœ¨additional_infoé‡Œæå‡ºéœ€è¦ä¾§é‡ç”¨è‹±æ–‡è¿›è¡ŒæŸ¥è¯¢ï¼Œä»¥è¿”å›žæ›´å¤šçš„è‹±æ–‡æ˜µç§°ã€‚
    4ã€å¦‚æžœcorrect_nicknamesé‡Œæœ‰è¶³å¤Ÿå¤šçš„å“ç‰Œæˆ–äº§å“è‹±æ–‡å’Œä¸­æ–‡åå­—ï¼ˆå·²ç»æ»¡è¶³ä¸Šé¢çš„éœ€æ±‚äº†ï¼‰ï¼Œè¯·è¾“å‡º[FINAL_RESULT]ï¼Œå’Œè¾“å‡ºå®Œæ•´çš„æ˜µç§°å’Œé“¾æŽ¥åˆ—è¡¨ã€‚
    5ã€è¯·ä¿æŒåˆç†çš„æŸ¥è¯¢æ¬¡æ•°ã€‚å¦‚æžœæŸ¥è¯¢æ¬¡æ•°å·²ç»è¶…è¿‡äº†5ï¼Œè¯·è¾“å‡º[FINAL_RESULT]ï¼Œå¹¶ä¾¿ç”¨çŽ°æœ‰çš„ã€æŸ¥è¯¢ç ”ç©¶ä»£ç†ã€‘ç½‘ç»œæœç´¢ç»“æžœæ•°æ®è¾“å‡ºä¸€ä¸ªå®Œæ•´çš„åå­—å’Œé“¾æŽ¥åˆ—è¡¨ã€‚
    '''

    chat_prompt = ChatPromptTemplate.from_messages([SystemMessage(content=system_prompt),
                                                    AIMessage(content=f'## ã€æŸ¥è¯¢ç ”ç©¶ä»£ç†ã€‘ç½‘ç»œæœç´¢ç»“æžœæ•°æ® å¦‚ä¸‹ï¼š\n\n{research_data}')])
    chain = chat_prompt | llm2 | parser

    analyst_response = chain.invoke({})
    next_action = analyst_response.model_dump().get('next_action')
    additional_info = analyst_response.model_dump().get('additional_info')
    result = analyst_response.model_dump().get('result')

    print(f"åˆ†æžå¸ˆæ€»ç»“: {next_action}\n\n{additional_info}\n\n{result}...")

    # Check if analyst wants more info or is ready to report
    if next_action == "[FINAL_RESULT]" or iteration >= 5:
        # Extract report (remove the [FINAL REPORT] marker if present)
        return {
            "final_result": result,
            "messages": [AIMessage(content=f"[ANALYST]: {result}", name="analyst")],
            "next_agent": "end",
            "iteration_count": iteration + 1
        }
    else:
        # Analyst needs more info
        return {
            "messages": [AIMessage(content=additional_info, name="analyst")],
            "next_agent": "researcher",
            "iteration_count": iteration + 1
        }


def route_agent(state: AgentState) -> Literal["researcher", "analyst", "end"]:
    """
    Route to the next agent based on state
    """
    next_agent = state.get("next_agent", "researcher")
    #
    # if next_agent == "end":
    #     return END
    return next_agent


# Build the graph
def create_research_graph():
    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("researcher", researcher_node)
    workflow.add_node("analyst", analyst_node)

    # Set entry point
    workflow.set_entry_point("researcher")

    # Add conditional edges
    workflow.add_conditional_edges(
        "researcher",
        route_agent,
        {
            "analyst": "analyst",
            "end": END
        }
    )

    workflow.add_conditional_edges(
        "analyst",
        route_agent,
        {
            "researcher": "researcher",
            "end": END
        }
    )

    return workflow.compile()


class run_research_report():
    def run(self,subject: str,industry: str):
        """
        Run the two-agent system to research and report on a subject
        """
        print(f"\n{'=' * 60}")
        print(f"Starting Name Research on industry ({industry}) - subject ({subject})")
        print(f"{'=' * 60}")

        graph = create_research_graph()

        initial_state = {
            "industry": industry,
            "subject": subject,
            "messages": [],
            "research_data": [],
            "final_result": "",
            "next_agent": "researcher",
            "iteration_count": 0
        }

        # Run the graph
        final_state = graph.invoke(initial_state)

        print(f"\n{'=' * 60}")
        print("FINAL RESULT")
        print(f"{'=' * 60}")
        print(final_state["final_result"])
        print(f"\n{'=' * 60}")

        return final_state

    def _save_result(self, subject: str, industry: str, result: str):
        """Save the report to a file"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{industry}_{subject}_result_{timestamp}.txt"
        filepath = settings.OUTPUT_DIR / filename
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"SUBJECT: {subject}\n")
            f.write(f"INDUSTRY: {industry}\n")
            f.write(f"DATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"\n{'=' * 70}\n\n")
            f.write(result)
        print(f"âœ… Result saved to: {filepath}")

    def _save_research_result(self, subject: str, industry: str, result: str):
        """Save the report to a file"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{industry}_{subject}_research_result_{timestamp}.txt"
        filepath = settings.OUTPUT_DIR / filename
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"SUBJECT: {subject}\n")
            f.write(f"INDUSTRY: {industry}\n")
            f.write(f"DATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"\n{'=' * 70}\n\n")
            f.write(result)
        print(f"âœ… Result saved to: {filepath}")

